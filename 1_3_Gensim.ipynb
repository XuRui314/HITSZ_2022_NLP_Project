{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XuRui314/HITSZ_2022_NLP_Project/blob/main/1_3_Gensim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUfgoHelMyTV"
      },
      "source": [
        "## 1.3 Gensim\n",
        "\n",
        "Gensim 是一个开源的python库，可以将文档表示为语义向量。\n",
        "\n",
        "官网：https://radimrehurek.com/gensim/\n",
        "\n",
        "- Word2vec\n",
        "- FastText\n",
        "- TF-IDF, LSA, LDA\n",
        "\n",
        "思考：为什么要把词表示为向量？\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeNDx7_9MyTZ",
        "outputId": "ccc67f03-5775-4f4f-e1ed-41853ed9d248"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_3979/1657659734.py:9: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
            "  glove2word2vec(gloveFile, word2vecFile)\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "# [\n",
        "#     'a':[1.2,,,],\n",
        "# 'b':[1,3,....]\n",
        "# ]\n",
        "def transfer(gloveFile, word2vecFile):\n",
        "    glove2word2vec(gloveFile, word2vecFile)\n",
        "\n",
        "transfer('./glove.6B.50d.txt','./glove.6B.50d_w2v_format.txt')\n",
        "#glove 和word2vec格式不同，进行转换"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4LRnLyjMyTb"
      },
      "outputs": [],
      "source": [
        "# 加载预训练的词向量\n",
        "glove_vectors = KeyedVectors.load_word2vec_format('./glove.6B.50d_w2v_format.txt', binary=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSvgAYMdMyTc",
        "outputId": "dbc1c3c4-828b-4b26-8916-2d95429b6f33"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('facebook', 0.9333045482635498),\n",
              " ('myspace', 0.8801369667053223),\n",
              " ('youtube', 0.8430657982826233),\n",
              " ('blog', 0.8262057304382324),\n",
              " ('blogs', 0.8064824342727661),\n",
              " ('blogging', 0.7970671057701111),\n",
              " ('tumblr', 0.7901089787483215),\n",
              " ('email', 0.778261125087738),\n",
              " ('tweets', 0.7604537010192871),\n",
              " ('e-mail', 0.7538726925849915)]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 查看与'twitter'最相近的词\n",
        "glove_vectors.most_similar('twitter')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbIuk_pyMyTc",
        "outputId": "f2b7bda8-f0e6-4d95-ff6c-ca070d410d40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.079084, -0.81504 ,  1.7901  ,  0.91653 ,  0.10797 , -0.55628 ,\n",
              "       -0.84427 , -1.4951  ,  0.13418 ,  0.63627 ,  0.35146 ,  0.25813 ,\n",
              "       -0.55029 ,  0.51056 ,  0.37409 ,  0.12092 , -1.6166  ,  0.83653 ,\n",
              "        0.14202 , -0.52348 ,  0.73453 ,  0.12207 , -0.49079 ,  0.32533 ,\n",
              "        0.45306 , -1.585   , -0.63848 , -1.0053  ,  0.10454 , -0.42984 ,\n",
              "        3.181   , -0.62187 ,  0.16819 , -1.0139  ,  0.064058,  0.57844 ,\n",
              "       -0.4556  ,  0.73783 ,  0.37203 , -0.57722 ,  0.66441 ,  0.055129,\n",
              "        0.037891,  1.3275  ,  0.30991 ,  0.50697 ,  1.2357  ,  0.1274  ,\n",
              "       -0.11434 ,  0.20709 ], dtype=float32)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 查看'computer'的词向量\n",
        "glove_vectors['computer']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PICDb4z9MyTd"
      },
      "source": [
        "### 下面使用文本中预训练的词向量进行情感分类"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1SlnSOFMyTd",
        "outputId": "bed3f82b-14fb-4698-e62c-975740e827e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: 1600\n",
            "test: 400\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "\n",
        "def load_movie_reviews():\n",
        "    pos_ids = movie_reviews.fileids('pos')\n",
        "    neg_ids = movie_reviews.fileids('neg')\n",
        "\n",
        "    all_reviews = []\n",
        "    for pids in pos_ids:\n",
        "        all_reviews.append((movie_reviews.raw(pids), 'positive'))\n",
        "    \n",
        "    for nids in neg_ids:\n",
        "        all_reviews.append((movie_reviews.raw(nids), 'negative'))\n",
        "\n",
        "    random.shuffle(all_reviews)\n",
        "    train_reviews = all_reviews[:1600]\n",
        "    test_reviews = all_reviews[1600:]\n",
        "\n",
        "    return train_reviews, test_reviews\n",
        "\n",
        "train_reviews, test_reviews = load_movie_reviews()\n",
        "print('train:', len(train_reviews))\n",
        "print('test:', len(test_reviews))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llAW53giMyTe"
      },
      "outputs": [],
      "source": [
        "from nltk import word_tokenize\n",
        "import numpy as np\n",
        "\n",
        "# 将文本中每个词的词向量的平均作为文本的表示\n",
        "def convert_text_to_vector(text, vectors):\n",
        "    vector = np.zeros(vectors.vector_size)\n",
        "    #最终文本向量的初始化\n",
        "    num = 0\n",
        "    for word in word_tokenize(text):\n",
        "        if word in vectors:\n",
        "            vector = vector + vectors[word]\n",
        "            num += 1\n",
        "    if num > 0:\n",
        "        vector = vector / num\n",
        "    return vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2g0KBi7MyTf"
      },
      "outputs": [],
      "source": [
        "def build_X_y(feature_extractor,reviews, vectors):\n",
        "    X = []\n",
        "    Y = []\n",
        "    \n",
        "    for review, polarity in reviews:\n",
        "        x = feature_extractor(review, vectors)\n",
        "        y = 0 if polarity == 'negative' else 1\n",
        "        X.append(x)\n",
        "        Y.append(y)\n",
        "\n",
        "    return X, Y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4Zim8PEMyTf"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = build_X_y(convert_text_to_vector,train_reviews, glove_vectors)\n",
        "X_test, y_test = build_X_y(convert_text_to_vector,test_reviews, glove_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmW3cp62MyTg",
        "outputId": "d219100b-53a5-4ab6-8252-15d3a453f6c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 2.82499822e-01  1.49704999e-01 -1.70585516e-01 -1.42463931e-01\n",
            "  3.47442902e-01  2.69994009e-01 -3.28432874e-01 -8.50855218e-02\n",
            " -2.36472232e-01 -2.44773272e-02  2.77545260e-02  9.46451616e-02\n",
            " -3.13645131e-01 -7.34175905e-02  4.54961444e-01  5.62172322e-02\n",
            "  3.91319665e-02  2.88562016e-02 -4.35868673e-01 -2.42483748e-01\n",
            " -8.23339735e-03  2.83226109e-01  2.35443052e-01  2.09568062e-03\n",
            "  2.50511249e-01 -1.39674038e+00 -3.81673463e-01  8.58117964e-02\n",
            "  1.93260298e-01 -1.87363802e-01  2.91190987e+00 -1.57550570e-02\n",
            " -1.11970683e-01 -3.05982004e-01  9.22510949e-02  3.67293589e-02\n",
            "  8.53569318e-02  1.76745555e-01  1.07471380e-01 -1.24025612e-01\n",
            " -1.07960290e-01  2.17176207e-01  6.30604254e-02 -5.81833029e-02\n",
            " -1.09076334e-01  8.35401807e-02 -5.80367818e-02 -2.57382441e-01\n",
            " -2.68450526e-02  3.43150741e-02]\n"
          ]
        }
      ],
      "source": [
        "print(X_test[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jugOeZILMyTg"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "#线性SVM\n",
        "\n",
        "def train_and_test(X_train, y_train, X_test, y_test):\n",
        "    classifier = LinearSVC()\n",
        "\n",
        "    classifier.fit(X_train, y_train)\n",
        "    accuracy = classifier.score(X_test, y_test)\n",
        "    print(f'accuracy is {accuracy:.4f}')\n",
        "\n",
        "    return classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "W1W4_2V1MyTg",
        "outputId": "dd511278-6dc7-4b2d-e425-192fcc20e413"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy is 0.7050\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LinearSVC()"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_and_test(X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6thU2btjMyTh"
      },
      "source": [
        "思考：进一步的改进\n",
        "\n",
        "1.优化特征提取方式？\n",
        "\n",
        "\n",
        "2.使用更优秀的分类器？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1eY6Q5MMyTh"
      },
      "outputs": [],
      "source": [
        "# 将文本中每个词的词向量的维度最大值作为文本的表示\n",
        "\n",
        "\n",
        "def convert_text_to_vector_max(text, vectors):\n",
        "    word_vectors = None\n",
        "    num = 0\n",
        "    for word in word_tokenize(text):\n",
        "        if word in vectors:\n",
        "            if word_vectors is None:\n",
        "                word_vectors = np.expand_dims(vectors[word], axis=0) \n",
        "            else:\n",
        "\n",
        "                word_vectors = np.concatenate((word_vectors, np.expand_dims(vectors[word], axis=0)),axis=0) \n",
        "            num += 1\n",
        "    # print(word_vectors.shape)\n",
        "\n",
        "    vector = word_vectors.max(axis=0)\n",
        "    return vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhPSZNlzMyTh",
        "outputId": "f7f92926-7837-4234-abf4-c2adf34aa7a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3,)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1, 3)"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = np.array([1,2,3])\n",
        "print(a.shape)\n",
        "a = np.expand_dims(a, axis=0)\n",
        "a.shape "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKLPIMtKMyTi"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = build_X_y(convert_text_to_vector_max,train_reviews, glove_vectors)\n",
        "X_test, y_test = build_X_y(convert_text_to_vector_max,test_reviews, glove_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MbUsBIRMyTi",
        "outputId": "8ec71097-c62b-4a53-cb34-f8707e06c899"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n"
          ]
        }
      ],
      "source": [
        "print(len(X_train[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yb2ZmSzEMyTi",
        "outputId": "cd62c1b7-babc-4fbd-9892-17b4cf66e265"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy is 0.6100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/wangrui/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LinearSVC()"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_and_test(X_train, y_train, X_test, y_test)\n",
        "#未收敛，方法有问题"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWvyTpjJMyTj"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import opinion_lexicon\n",
        "# 导入情感词典\n",
        "positive_words = set(opinion_lexicon.positive())\n",
        "negative_words = set(opinion_lexicon.negative())\n",
        "\n",
        "# 将只考虑情感词\n",
        "\n",
        "def convert_text_to_vector_senti(text, vectors):\n",
        "    vector = np.zeros(vectors.vector_size)\n",
        "    #最终文本向量的初始化\n",
        "    num = 0\n",
        "    for word in word_tokenize(text):\n",
        "        if word in vectors and (word in positive_words or word in negative_words):\n",
        "            vector = vector + vectors[word]\n",
        "            num += 1\n",
        "    if num > 0:\n",
        "        vector = vector / num\n",
        "    return vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6O4jjqtMyTj"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def convert_text_to_vector_sum(text, vectors):\n",
        "    vector = np.zeros(vectors.vector_size)\n",
        "    #最终文本向量的初始化\n",
        "    num = 0\n",
        "    for word in word_tokenize(text):\n",
        "        if word in vectors and (word in positive_words or word in negative_words):\n",
        "            vector = vector + vectors[word]\n",
        "    return vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KzZCxj9MyTj"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = build_X_y(convert_text_to_vector_senti,train_reviews, glove_vectors)\n",
        "X_test, y_test = build_X_y(convert_text_to_vector_senti,test_reviews, glove_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qokJsmr2MyTk",
        "outputId": "e88738fe-65f3-43c3-c893-24681d89f06b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy is 0.7525\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LinearSVC()"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_and_test(X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhCNhaNGMyTk"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QS6kiLCHMyTk",
        "outputId": "8f009ae6-60ef-4318-ab2d-5d3e3b6a741e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy is 0.5550\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/wangrui/miniconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LinearSVC()"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22grUWVbMyTk"
      },
      "source": [
        "其他优化特征表示的方式：\n",
        "\n",
        "\n",
        "1.使用更高维度的词向量，100，200，300？\n",
        "\n",
        "\n",
        "2.使用预训练更加充分的词向量，在更大语料库上训练得到的词向量\n",
        "\n",
        "\n",
        "3.使用其他方式对词向量进行处理，不只是简单的平均，如：使用CNN，多核CNN以注意到不同尺度的词汇间的信息，加上RNN对词向量编码句意信息\n",
        "\n",
        "3.1 再对RNN进行优化？LSTM，BiLSTM?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SViI3f-MyTk"
      },
      "source": [
        "#### transformer\n",
        "\n",
        "参考资料\n",
        "- http://jalammar.github.io/illustrated-transformer/\n",
        "- http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
        "- https://arxiv.org/abs/1706.03762"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0WNkt2VMyTl"
      },
      "source": [
        "下周：\n",
        "\n",
        "\n",
        "### 1. Pytorch\n",
        "\n",
        "官网：https://pytorch.org/\n",
        "\n",
        "- tensor\n",
        "- 自动求导\n",
        "- 深度学习流程\n",
        "    - 加载数据\n",
        "    - 建立模型\n",
        "    - 选择优化器\n",
        "    - 训练\n",
        "    - 保存和加载模型\n",
        "\n",
        "参考：\n",
        "- https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html\n",
        "- https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
        "\n",
        "### 2.NER任务baseline\n",
        "\n",
        "- 弱baseline：BiLSTM+CRF\n",
        "\n",
        "- 强baseline：BERT+CRF\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "1.3 Gensim.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}